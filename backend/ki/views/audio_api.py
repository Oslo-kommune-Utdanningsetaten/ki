import os
import asyncio
import json
from azure.cognitiveservices.speech import SpeechConfig, AudioConfig, SpeechRecognizer, SpeechSynthesizer, AudioDataStream, SpeechSynthesisOutputFormat
from azure.cognitiveservices.speech.audio import PushAudioInputStream
from channels.generic.websocket import AsyncWebsocketConsumer
from ki.views.ai_providers.azure import chat_completion_azure
from asgiref.sync import sync_to_async


# consider using a websocket connection to Azure
# https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/samples/python/tts-text-stream/text_stream_sample.py

# serverStatus is one of: 'websocketOpened', 'websocketClosed','initializing', 'streamingAudioToAzure', 'streamingTextToClient', 'generatingChatResponse', 'generatingAudioResponse', 'streamingAudioToClient', 'ready'

class AudioConsumer(AsyncWebsocketConsumer):

    # Called when the WebSocket connection is opened
    async def connect(self):
        await self.accept()

        # Initialize messages list
        self.messages = []
        self.bot_model = None
        self.bot_uuid = None
        self.current_server_status = None
        self.speech_recognizer = None
        self.speech_synthesizer = None
        await self.send_server_status("websocketOpened")

        # Create a push stream
        self.push_stream_audio = PushAudioInputStream()

        # Create an audio configuration using the push stream
        self.audio_input_config = AudioConfig(stream=self.push_stream_audio)

        # Define speech config
        self.speech_config = SpeechConfig(subscription=os.environ.get('AZURE_SPEECH_KEY'), region=os.environ.get('AZURE_SPEECH_REGION'))


    # Called when the WebSocket connection is closed
    async def disconnect(self, close_code):
        await self.send_server_status("websocketClosed")
        self.speech_recognizer.stop_continuous_recognition()
        self.push_stream_audio.close()
        self.speech_synthesizer.stop_speaking()
        await self.close()


    # Called when the WebSocket receives a message
    async def receive(self, text_data=None, bytes_data=None):
        if text_data:
            # Client is sending text data. This only happens at initialization and later when language or voice is changed
            data = json.loads(text_data)
            if data.get('bot_uuid'):
                self.bot_uuid = data.get('bot_uuid')
            if data.get('messages'):
                self.messages = data.get('messages')
            if data.get('bot_model'):
                self.bot_model = data.get('bot_model')
    
            await self.maybe_initialize(data.get('selected_language'), data.get('selected_voice'))

        if bytes_data:
            try:
                # Log incoming data for debugging
                #print(f"Received audio frame of size: {len(bytes_data)} bytes")
                
                # Client is sending audio data generated by the user
                await self.send_server_status("streamingAudioToAzure")
                
                # Write to outbound stream for transcription by Azure
                self.push_stream_audio.write(bytes_data)
            except Exception as e:
                print(f"Error while processing audio data: {e}")
                #await self.close()  # Optionally close the connection gracefully

    def recognized_callback(self, evt):
        # Received transcript from Azure
        recognized_text = evt.result.text
        if recognized_text:
            print(f"Received transcript: {recognized_text}")

            # For cosmetic reasons, if recognized_text includes only one period, remove it
            if recognized_text.count('.') == 1:
                recognized_text = recognized_text.strip('.')

            self.messages.append({
                "role": "user",
                "content": recognized_text
            })

            self.send_server_status("streamingTextToClient")

            # Send updated messages to client
            asyncio.run(self.send(text_data=json.dumps({
                "type": "websocket.text",
                "messages": self.messages
            })))

            # Request completion based on messages
            self.send_server_status("generatingChatResponse")
            completion = asyncio.run(chat_completion_azure(self.messages, {"bot_model": self.bot_model}))
            print(f"Generated completion: {completion}")

            # Synthesize completion and stream audio to client
            asyncio.run(self.synthesize_and_stream(completion))

            # Append completion to messages
            self.messages.append({
                "role": "assistant",
                "content": completion
            })

            # Send updated messages to client
            asyncio.run(self.send(text_data=json.dumps({
                "type": "websocket.text",
                "messages": self.messages
            })))


    async def synthesize_and_stream(self, textInput):
        await self.send_server_status("generatingAudioResponse")

        try:
            result = await sync_to_async(self.speech_synthesizer.speak_text)(textInput)
        except Exception as e:
            print(f"Error during speech synthesis: {e}")
            return
        
        audio_stream = AudioDataStream(result)

        # send start signal to client
        await self.send(text_data=json.dumps({
            "type": "websocket.audio",
            "command": "audio-stream-begin"
        }))

        # Stream until no more data
        await self.send_server_status("streamingAudioToClient")
        try:
            buffer = bytes(16000)
            filled_size = audio_stream.read_data(buffer)
            while filled_size > 0:
                await self.send(bytes_data=buffer[:filled_size])
                filled_size = audio_stream.read_data(buffer)
        except Exception as e:
            print(f"Error while streaming audio to client: {e}")

        # send stop signal to client
        await self.send(text_data=json.dumps({
            "type": "websocket.audio",
            "command": "audio-stream-end"
        }))
        await self.send_server_status("ready")


    async def maybe_initialize(self, language, voice):
        if language and voice and (language != self.speech_config.speech_recognition_language or voice != self.speech_config.speech_synthesis_voice_name):
            print(f"Initializing with language: {language} and voice: {voice}")
            await self.send_server_status("initializing")

            if self.speech_recognizer:
                self.speech_recognizer.stop_continuous_recognition()
            if self.speech_synthesizer:
                self.speech_synthesizer.stop_speaking()
    
            # Initialize speech recognizer
            self.speech_config.speech_recognition_language=language
            self.speech_recognizer = SpeechRecognizer(speech_config=self.speech_config, audio_config=self.audio_input_config)
            # Register speech recognizer callbacks
            self.speech_recognizer.recognized.connect(self.recognized_callback)
            self.speech_recognizer.session_stopped.connect(self.stop_callback)
            self.speech_recognizer.session_started.connect(self.start_callback)
            self.speech_recognizer.canceled.connect(self.canceled_callback)
            self.speech_recognizer.speech_start_detected.connect(self.speech_start_callback)
            self.speech_recognizer.speech_end_detected.connect(self.speech_end_callback)
            # Start recognition immediately
            self.speech_recognizer.start_continuous_recognition()

            # Initialize speech synthesizer
            self.speech_config.speech_synthesis_voice_name=voice
            self.speech_config.set_speech_synthesis_output_format(
                SpeechSynthesisOutputFormat.Audio16Khz128KBitRateMonoMp3
            )
            self.speech_synthesizer = SpeechSynthesizer(speech_config=self.speech_config, audio_config=None)
            await self.send_server_status("ready")


    async def send_server_status(self, status):
        if self.current_server_status == status:
            return
        self.current_server_status = status
        await self.send(text_data=json.dumps({
            "type": "websocket.text",
            "serverStatus": status
        }))

    def speech_start_callback(self, evt):
        print(f"Recognition start")

    def speech_end_callback(self, evt):
        print(f"Recognition end")

    def stop_callback(self, evt):
        print(f"Recognition stopped")

    def start_callback(self, evt):
        print("Ready and awaiting audio stream from client")

    def canceled_callback(self, evt):
        print(f"Recognition canceled: {evt.reason}")