import os
import asyncio
import json
from .. import models
from azure.cognitiveservices.speech import SpeechConfig, AudioConfig, SpeechRecognizer, SpeechSynthesizer, AudioDataStream, SpeechSynthesisOutputFormat
from azure.cognitiveservices.speech.audio import PushAudioInputStream
from channels.generic.websocket import AsyncWebsocketConsumer
from ki.views.ai_providers.azure import chat_completion_azure
from asgiref.sync import sync_to_async


# consider using a websocket connection to Azure
# https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/samples/python/tts-text-stream/text_stream_sample.py

class AudioConsumer(AsyncWebsocketConsumer):

    # Called when the WebSocket connection is opened
    async def connect(self):
        await self.accept()

        # Initialize messages list
        self.messages = []
        self.bot_model = None
        self.bot_uuid = None

        # Create a push stream
        self.push_stream_audio = PushAudioInputStream()

        # Create an audio configuration using the push stream
        self.audio_input_config = AudioConfig(stream=self.push_stream_audio)

        # Initialize speech recognizer with retries
        retries = 3
        while retries > 0:
            try:
                self.initialize("nb-NO", 'nb-NO-FinnNeural')
                break

            except Exception as e:
                print(f"Error initializing speech recognizer: {e}")
                retries -= 1
                if retries == 0:
                    await self.close()


    # Called when the WebSocket connection is closed
    async def disconnect(self, close_code):
        self.speech_recognizer.stop_continuous_recognition()
        self.push_stream_audio.close()
        self.speech_synthesizer.stop_speaking()
        await self.close()


    # Called when the WebSocket receives a message
    async def receive(self, text_data=None, bytes_data=None):
        if text_data:
            print(f"00 - Received text data: {text_data}")
            # client is sending text data
            data = json.loads(text_data)
            if data.get('bot_uuid'):
                self.bot_uuid = data.get('bot_uuid')
            if data.get('messages'):
                self.messages = data.get('messages')
            if data.get('bot_model'):
                self.bot_model = data.get('bot_model')
            if data.get('selected_language') and data.get('selected_voice'):
                self.initialize(data.get('selected_language'), data.get('selected_voice'))

            print(f"01 - Received messages from {self.bot_uuid} // {self.bot_model} : {self.messages}")

        if bytes_data:
            # Client is sending audio data generated by the user
            #print(f"02 - Received audio from {self.bot_uuid}")
            # Write to outbound stream for transcription by Azure
            self.push_stream_audio.write(bytes_data)


    def recognized_callback(self, evt):
        recognized_text = evt.result.text
        # Received transcript from Azure
        if recognized_text:
            # Prepare a slot on the messages array
            self.messages.append({
                "role": "user",
                "content": ''
            })

            # For cosmetic reasons, if recognized_text includes only one period, remove it
            if recognized_text.count('.') == 1:
                recognized_text = recognized_text.strip('.')

            self.messages[-1]['content'] = recognized_text
            print(f"03 - Received trascript: {recognized_text}")

            # Send updated messages to client
            asyncio.run(self.send(text_data=json.dumps({
                "type": "websocket.text",
                "messages": self.messages
            })))
            print(f"04 - Messages with trascript sent to client: {recognized_text}")

            # Request completion based on messages
            completion = asyncio.run(chat_completion_azure(self.messages, {"bot_model": self.bot_model}))
            print(f"05 - Generated completion based on transcript: {completion}")

            # Synthesize completion and stream audio to client
            asyncio.run(self.synthesize_and_stream(completion))

            print(f"11 - Synth is officially done")

            # Append completion to messages
            self.messages.append({
                "role": "assistant",
                "content": completion
            })

            print(f"12 - Now send update messages...")

            # Send updated messages to client
            asyncio.run(self.send(text_data=json.dumps({
                "type": "websocket.text",
                "messages": self.messages
            })))

            print(f"13 - Messages sent to client, all done")



    async def synthesize_and_stream(self, textInput):
        print(f"06 - Synthesizing: {textInput}")

        try:
            result = await sync_to_async(self.speech_synthesizer.speak_text)(textInput)
            print(f"07 - Got synth, duration: {result.audio_duration}")
        except Exception as e:
            print(f"Error during speech synthesis: {e}")
            return
        
        audio_stream = AudioDataStream(result)
        print(f"08 - Will send start signal to client")
        # send start signal to client
        await self.send(text_data=json.dumps({
            "type": "websocket.audio",
            "status": "start"
        }))
        print(f"09 - Sent! Now beginning stream...")
        # Stream until no more data
        try:
            buffer = bytes(16000)
            filled_size = audio_stream.read_data(buffer)
            while filled_size > 0:
                await self.send(bytes_data=buffer[:filled_size])
                filled_size = audio_stream.read_data(buffer)
        except Exception as e:
            print(f"Error while streaming audio to client: {e}")

        print(f"10 - Stream done, now send stop signal...")
        # send stop signal to client
        await self.send(text_data=json.dumps({
            "type": "websocket.audio",
            "status": "stop"
        }))
        print(f"11 - Stop signal sent!")


    def initialize(self, language_code, voice_code):
        print(f"-----> Initializing with {language_code} and {voice_code}")
        self.speech_config = SpeechConfig(subscription=os.environ.get('AZURE_SPEECH_KEY'), region=os.environ.get('AZURE_SPEECH_REGION'), speech_recognition_language=language_code)
        #self.speech_config.speech_recognition_language=language_code
        self.speech_recognizer = SpeechRecognizer(speech_config=self.speech_config, audio_config=self.audio_input_config)
        # Register callbacks
        self.speech_recognizer.recognized.connect(self.recognized_callback)
        self.speech_recognizer.session_stopped.connect(self.stop_callback)
        self.speech_recognizer.session_started.connect(self.start_callback)
        self.speech_recognizer.canceled.connect(self.canceled_callback)

        # Start recognition immediately
        self.speech_recognizer.start_continuous_recognition()
        self.speech_config.speech_synthesis_voice_name=voice_code
        self.speech_config.set_speech_synthesis_output_format(
            SpeechSynthesisOutputFormat.Audio16Khz128KBitRateMonoMp3
        )
        self.speech_synthesizer = SpeechSynthesizer(speech_config=self.speech_config, audio_config=None)


    def stop_callback(self, evt):
        print("Recognition stopped.")

    def start_callback(self, evt):
        print("Recognition started!")

    def canceled_callback(self, evt):
        print(f"Recognition canceled: {evt.reason}")